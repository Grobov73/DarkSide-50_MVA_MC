{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code for blur $^{40}Ar$ Monte Carlo parameters\n",
    "# Код для размытия Монте Карло данных $^{40}Ar$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries\n",
    "### Подключаем необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import uproot\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from matplotlib import pylab as plt\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "import math\n",
    "from scipy.special import gamma\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import numpy.random as ra\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "import cellbell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import datasets \n",
    "### Импортируем наборы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_Ar40_m100 = pd.read_csv('/home/kingaa/Desktop/DarkSide/Monte/data/outputWIMP_400kWIMP_m100.csv',header = 0, sep = \",\")\n",
    "ds_Ar40_m200 = pd.read_csv('/home/kingaa/Desktop/DarkSide/Monte/data/outputWIMP_400kWIMP_m200.csv',header = 0, sep = \",\")\n",
    "ds_Ar40_m500 = pd.read_csv('/home/kingaa/Desktop/DarkSide/Monte/data/outputWIMP_400kWIMP_m500.csv',header = 0, sep = \",\")\n",
    "ds_Ar40_m1000 = pd.read_csv('/home/kingaa/Desktop/DarkSide/Monte/data/outputWIMP_400kWIMP_m1000.csv',header = 0, sep = \",\")\n",
    "\n",
    "ds_Ar40_m100 = ds_Ar40_m100.dropna()\n",
    "ds_Ar40_m200 = ds_Ar40_m200.dropna()\n",
    "ds_Ar40_m500 = ds_Ar40_m500.dropna()\n",
    "ds_Ar40_m1000 = ds_Ar40_m1000.dropna()\n",
    "\n",
    "ds_Ar40_mall = pd.concat([ds_Ar40_m100, ds_Ar40_m200, ds_Ar40_m500, ds_Ar40_m1000])\n",
    "\n",
    "ds_Ar40 = ds_Ar40_mall\n",
    "ds_Ar40 = ds_Ar40.query('s1>30 and s1 < 500 and f90 > 0.5 and f90 < 0.82')\n",
    "\n",
    "Ar40_f90 = list(ds_Ar40['f90'])\n",
    "Ar40_f30 = list(ds_Ar40['f30'])\n",
    "Ar40_f60 = list(ds_Ar40['f60'])\n",
    "Ar40_f200 = list(ds_Ar40['f200'])\n",
    "Ar40_s1 = list(ds_Ar40['s1'])\n",
    "Ar40_s2 = list(ds_Ar40['s2'])\n",
    "Ar40_late = list(ds_Ar40['s1late'])\n",
    "Ar40_prompt = list(ds_Ar40['s1prompt'])\n",
    "\n",
    "ds_AmBe = \"/home/kingaa/Desktop/DarkSide/Monte/AmBe_cut_500.root\"\n",
    "tree_AmBe = uproot.open(ds_AmBe)[\"TreeB\"]\n",
    "\n",
    "df_f90_ds_AmBe = tree_AmBe.pandas.df('f90')\n",
    "df_f90_ds_AmBe['f30'] = tree_AmBe.pandas.df('f30')\n",
    "df_f90_ds_AmBe['f60'] = tree_AmBe.pandas.df('f60')\n",
    "df_f90_ds_AmBe['f200'] = tree_AmBe.pandas.df('f200')\n",
    "df_f90_ds_AmBe['s1'] = tree_AmBe.pandas.df('s1')\n",
    "df_f90_ds_AmBe['s2'] = tree_AmBe.pandas.df('s2')\n",
    "df_f90_ds_AmBe['sprompt'] = tree_AmBe.pandas.df('long_prompt')\n",
    "df_f90_ds_AmBe['slate'] = tree_AmBe.pandas.df('long_late')\n",
    "df_f90_ds_AmBe = df_f90_ds_AmBe.query('s1>30 and s1 < 500 and f90 > 0.5 and f90 < 0.82')\n",
    "\n",
    "AmBe_f90 = list(df_f90_ds_AmBe['f90'])\n",
    "AmBe_f30 = list(df_f90_ds_AmBe['f30'])\n",
    "AmBe_f60 = list(df_f90_ds_AmBe['f60'])\n",
    "AmBe_f200 = list(df_f90_ds_AmBe['f200'])\n",
    "AmBe_s1 = list(df_f90_ds_AmBe['s1'])\n",
    "AmBe_s2 = list(df_f90_ds_AmBe['s2'])\n",
    "AmBe_sprompt = list(df_f90_ds_AmBe['sprompt'])\n",
    "AmBe_slate = list(df_f90_ds_AmBe['slate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit functions creation\n",
    "### Создаем фитирующие функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEAP function\n",
    "#### Функция распределения коллаборации DEAP\n",
    "##### DEAP collaboration \"Search for dark matter with a 231-day exposure of liquid argon using DEAP-3600 at SNOLAB\", 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deap_distr_NR(f, a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, A):\n",
    "    global q \n",
    "    fbar = a0 + a1/(q - a2) + a3/(q - a4)**2\n",
    "    fbarnew = 1 - fbar\n",
    "    b = a5 + a6/q + a7/q**2\n",
    "    sigma = a8 + a9/q + a10/q**2\n",
    "    fnr = A * 1/math.sqrt(2*math.pi*sigma**2) * np.exp(-(f**2)/(2*sigma**2)) * (fbar**b * (1 - f)**(b-1) * np.exp(-fbar*(1 - f)))/(gamma(b))\n",
    "    #fnr = 1/np.sqrt(2*math.pi*sigma**2) * np.exp(-(f**2)/(2*sigma**2)) \n",
    "    return fnr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### David Hinkley function \n",
    "#### Функция распределения Дэвида Хинкли\n",
    "##### W. H. Lippincott et. al. \"Scintillation time dependence and pulse shape discrimination in liquid argon\", 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinkley(x, mu_s1late_NR, mu_s1prompt_NR, sigma_s1late_NR,sigma_s1prompt_NR, A):\n",
    "    func1 = A * np.exp((-0.5*(mu_s1late_NR*x-mu_s1prompt_NR*(1-x))**2)/(sigma_s1late_NR**2*x**2+sigma_s1prompt_NR**2*(1-x)**2))*(sigma_s1late_NR**2*mu_s1prompt_NR*x+sigma_s1prompt_NR**2*mu_s1late_NR*(1-x))/((2*np.pi)**(0.5)*(sigma_s1late_NR**2*x**2+sigma_s1prompt_NR**2*(1-x)**2)**1.5)\n",
    "    return func1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code for $f_{30}$, $f_{60}$, $f_{90}$, $f_{200}$\n",
    "### Основной код для $f_{30}$, $f_{60}$, $f_{90}$, $f_{200}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform sampling for hinkley distribution\n",
    "# Создаем функцию выборки обратного преобразования\n",
    "def its_hinkley_NR(N):\n",
    "    prob = f/float(sum(f))\n",
    "    cum_prob = np.cumsum(prob)   \n",
    "    R = ra.uniform(0, 1, N)\n",
    "    gen_random = [float(x[np.argwhere(cum_prob == min(cum_prob[(cum_prob - r) > 0]))]) for r in R] \n",
    "    return gen_random\n",
    "# Blur function for 'extra values' - values that gets into a crowded bin\n",
    "# Создаем функцию размытия для 'экстра значений' - значений, которые попали в переполненный бин\n",
    "def blur(value, depth):\n",
    "    j = int(np.argwhere(list_AmBe_data[:100] == max(list_AmBe_data[:100][(list_AmBe_data[:100] - value) <= 0])))\n",
    "    if (n_list[j] < n_AmBe_data[j]) or depth == 10:\n",
    "        return j, value\n",
    "    else:\n",
    "        depth = depth + 1\n",
    "        a = its_hinkley_NR(1)[0]\n",
    "        return blur(a, depth)\n",
    "# Function for comparison plotting data before and after blur\n",
    "# Функция для сравнительного построения данных до и после размытия\n",
    "def plotting (AmBe_data, Ar40_old, Ar40_new):\n",
    "    nbins = 50\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(121)\n",
    "    n_data_aar, list_data_aar, patches_data_aar = plt.hist(AmBe_data, bins = nbins, density = True, histtype = 'step', color = 'black', linewidth = 3, label = 'AAr')\n",
    "    n_data_ar40, list_data_ar40, patches_data_ar40 = plt.hist(Ar40_old, bins = nbins, density = True, histtype = 'step', color = 'red', linewidth = 3, label = 'Ar39')\n",
    "    plt.title('$^{39}$Ar $f_{30}$ до преобразований', fontsize=22)\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    n_data_aar, list_data_aar, patches_data_aar = plt.hist(AmBe_data, bins = nbins, density = True, histtype = 'step', color = 'black', linewidth = 3, label = 'AAr')\n",
    "    n_new_data_ar40, list_new_data_ar40, patches_new_data_ar40 = plt.hist(Ar40_new, bins = nbins, density = True, histtype = 'step', color = 'red', linewidth = 3, label = 'Ar39')\n",
    "    plt.title('$^{39}$Ar $f_{30}$ после преобразований', fontsize=22)\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_AmBe_list = ['AmBe_f30', 'AmBe_f60', 'AmBe_f90', 'AmBe_f200', 'AmBe_s1']\n",
    "Ar40_new_parameters = []\n",
    "global n_list\n",
    "for parameter in parameter_AmBe_list: \n",
    "    global data\n",
    "    n_AmBe_data, list_AmBe_data, patches_AmBe_data = plt.hist(parameter, bins = 100, density = False)\n",
    "    popt_AmBe, pcov_AmBe = curve_fit(hinkley, list_AmBe_data[:100], n_AmBe_data, maxfev=100000)\n",
    "    n_AmBe_data_fit = hinkley(list_AmBe_data, *popt_AmBe)\n",
    "    global x, f\n",
    "    x = np.linspace(min(parameter), max(parameter), 200)\n",
    "    f = hinkley(x, *popt_AmBe)\n",
    "    if (parameter == 'AmBe_f30'):\n",
    "        data = Ar40_f30\n",
    "    elif (parameter == 'AmBe_f60'):\n",
    "        data = Ar40_f60\n",
    "    elif (parameter == 'AmBe_f90'):\n",
    "        data = Ar40_f90\n",
    "    elif (parameter == 'AmBe_f200'):\n",
    "        data = Ar40_f200\n",
    "    data1 = data[:len(AmBe_f90)]\n",
    "    data2 = data[len(AmBe_f90):2*len(AmBe_f90)]\n",
    "    data3 = data[2*len(AmBe_f90):3*len(AmBe_f90)]\n",
    "    data_lists = [data1, data2, data3]\n",
    "    for data in data_lists:\n",
    "        data_new = []\n",
    "        n_list = [0] * 101\n",
    "        for i in range(len(data)):\n",
    "            j, value = blur(data[i], 0)\n",
    "            n_list[j] = n_list[j] + 1\n",
    "            data_new.append(value)\n",
    "        new_param = list(chain(*data_new))\n",
    "    Ar40_new_parameters.append(new_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code for $S_1$\n",
    "### Основной код для $S_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deap_random_hinkley_NR_s1(N):\n",
    "    prob = f_hinkley_NR/float(sum(f_hinkley_NR))\n",
    "    cum_prob = np.cumsum(prob)    \n",
    "    R = ra.uniform(0, 1, N)\n",
    "    gen_random = [float(x_hinkley_NR[np.argwhere(cum_prob == min(cum_prob[(cum_prob - r) > 0]))]) for r in R]  \n",
    "    return gen_random\n",
    "def blur_s1(value, depth):\n",
    "    if value == min(list_s1_AmBe):\n",
    "        value = value + 1\n",
    "    if value == max(list_s1_AmBe):\n",
    "        value = value - 1\n",
    "    j = int(np.argwhere(list_s1_AmBe[:200] == max(list_s1_AmBe[:200][(list_s1_AmBe[:200] - value) <= 0])))\n",
    "    if (n_list[j] < n_s1_AmBe_trial_deap[j]) or depth == 5:\n",
    "        return j, value\n",
    "    else:\n",
    "        depth = depth + 1\n",
    "        a = deap_random_hinkley_NR_s1(1)[0]\n",
    "        return razmitie_s1(a, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 'n_AAr_data' - number of events in each bin in AmBe data\n",
    "# Ищем чему равно n в каждом бине в реальных данных NR\n",
    "global n_s1_AmBe, list_s1_AmBe, patches_s1_AmBe\n",
    "n_s1_AmBe, list_s1_AmBe, patches_s1_AmBe = plt.hist(AmBe_s1, 200)\n",
    "# Fit\n",
    "# фитируем\n",
    "popt_s1_hinkley_AmBe, pcov_s1_hinkley_AmBe = curve_fit(hinkley, list_s1_AmBe[:200], n_s1_AmBe, maxfev=1000000)\n",
    "# Calculate n for fit function in each bin\n",
    "# вычисляем n для фитаglobal x_hinkley_NR,f_hinkley_NR\n",
    "x_hinkley_NR = np.linspace(min(Ar40_s1), max(Ar40_s1), 200)\n",
    "f_hinkley_NR = hinkley(x_hinkley_NR, *popt_s1_hinkley_AmBe)\n",
    "# Divide Ar40 dataset on 3 different datasets\n",
    "# Разделяем набор данных Ar40 на 3 различных набора данных\n",
    "data1_s1 = Ar40_s1[:len(AmBe_f90)]\n",
    "data2_s1 = Ar40_s1[len(AmBe_f90):2*len(AmBe_f90)]\n",
    "data3_s1 = Ar40_s1[2*len(AmBe_f90):3*len(AmBe_f90)]\n",
    "data_s1_list = [data1_s1, data2_s1, data3_s1]\n",
    "global n_list\n",
    "a = []\n",
    "for data in data_s1_list:\n",
    "    data_new = []\n",
    "    n_list = [0] * 201\n",
    "    for i in range (len(data)):\n",
    "        j, value = blur_s1(data[i], 0)\n",
    "        n_list[j] = n_list[j] + 1\n",
    "        data_new.append(value)\n",
    "    a.append(data_new)\n",
    "Ar40_new_s1 = list(chain(*a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pandas DataFrame and save it\n",
    "# Создание нового датафрейма и сохранение его в .csv файл\n",
    "data = {'f30':Ar40_new_parameters[0], 'f60':Ar40_new_parameters[1], 'f90':Ar40_new_parameters[2], 'f200':Ar40_new_parameters[3], 's1':Ar40_new_s1}\n",
    "df = pd.DataFrame(data) \n",
    "df.to_csv('new_ds_Ar40_MC_30_500_f30_f60_f90_f200_s1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
